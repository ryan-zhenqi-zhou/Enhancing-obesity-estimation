{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d720311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is for processing Safegraph Data, only 1 input, output tidied up place visitation data with the number of visitors.\n",
    "# Take \"Original Safegraph Data for Dataset 3 (Whole USA) ———— Fitness center\" for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f677d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d9df268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process pattern dataset\n",
    "def pattern_process(df, name, filenames1, folder_name):\n",
    "    print(\"Start pattern process \" + name)\n",
    "    # delete NA\n",
    "    df_tract_visitor = df['visitor_home_aggregation']\n",
    "    df_tract_visitor.rename(\"visitors\", inplace = True)\n",
    "    df_tract_visitor.dropna(inplace = True)\n",
    "    # delete {}\n",
    "    df_tract_visitor1 = pd.DataFrame(df_tract_visitor)\n",
    "    df_tract_visitor2 = df_tract_visitor1[~df_tract_visitor1['visitors'].isin(['{}'])]\n",
    "    # fix format error, like 32432432“”：4\n",
    "    df_tract_visitor3 = df_tract_visitor2[df_tract_visitor2[\"visitors\"].str.contains('\"\"')]\n",
    "    df_tract_visitor3.reset_index(drop = True, inplace = True)\n",
    "    for i in range(len(df_tract_visitor3)):\n",
    "        df_tract_visitor3[\"visitors\"][i] = \"{\" + '\"' + df_tract_visitor3[\"visitors\"][i][0:11] + '\"' + \":\" + df_tract_visitor3[\"visitors\"][i].split(\":\")[1] + \"}\"\n",
    "    df_tract_visitor4 = df_tract_visitor2[~df_tract_visitor2[\"visitors\"].str.contains('\"\"')]\n",
    "    df_tract_visitor5 = pd.concat([df_tract_visitor3, df_tract_visitor4])\n",
    "    df_tract_visitor5.reset_index(drop = True, inplace = True)\n",
    "    # test whether or not there will be same census tract, the answer is not, but I just keep it\n",
    "    for p in range(len(df_tract_visitor5)):\n",
    "        test_same_value = df_tract_visitor5[\"visitors\"][p] \n",
    "        test_same_value1 = re.findall(r'\"([^\"]*)\"', test_same_value)\n",
    "        set_lst = set(test_same_value1)\n",
    "        if len(set_lst) != len(test_same_value1):\n",
    "            print(name + \" ———> \" + \"pattern: \" + 'Same value, wrong!!!')\n",
    "    # dict to dataframe\n",
    "    list_data = df_tract_visitor5['visitors'].values.tolist()\n",
    "    dfss = []\n",
    "    for x in range(len(list_data)):\n",
    "        data_visitor = pd.DataFrame.from_dict(eval(list_data[x]), orient = 'index', columns = ['values'])\n",
    "        dfss.append(data_visitor)\n",
    "    data_visitor_f = pd.concat(dfss)\n",
    "    global data_visitor1\n",
    "    data_visitor1 = data_visitor_f.reset_index()\n",
    "    data_visitor1.rename(columns={'index':'tractcode','values':'visitors'},inplace=True) \n",
    "    # test whether or not they are all census tract, with the 11 number of digits\n",
    "    for y in range(len(data_visitor1)):\n",
    "        if len(data_visitor1['tractcode'][y]) != 11:\n",
    "            print(name + \" ———> \" + \"pattern: \" + str(y) + \"is not tract!!!\")\n",
    "    # data_visitor1.to_csv('{}/{}pattern{}.csv'.format(filenames1, folder_name, filenames1.split(\"2018\")[1].split(\"-2022-\")[0]),encoding='utf_8_sig',index=False)\n",
    "    return data_visitor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f756bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve estimation\n",
    "def curve_estimation(dfc, filename_input, folder_name):\n",
    "    print(\"begin curve estimation!\")\n",
    "    df1 = dfc.groupby(\"visitors\").count()\n",
    "    df1.reset_index(inplace = True)\n",
    "    df2 = df1[df1[\"visitors\"] != 4]\n",
    "    df2.rename(columns={'vsitors':'vsitors','tractcode':'counts'},inplace=True) \n",
    "    \n",
    "    xdata = np.array(df2[\"visitors\"])\n",
    "    ydata = np.array(df2[\"counts\"])\n",
    "    \n",
    "    def func(x,a,b):\n",
    "        return a*x**b\n",
    "    \n",
    "    popt, pcov = curve_fit(func, xdata, ydata)\n",
    "    residuals = ydata - func(xdata, *popt)\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((ydata-np.mean(ydata))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    fol = {\"a\" : popt[0],\n",
    "       \"b\" : popt[1],\n",
    "       \"R squared\" : r_squared}\n",
    "    fol1 = pd.DataFrame([fol])\n",
    "    fol1.to_csv('{}/{}curve_estimation_formula.csv'.format(filename_input, folder_name),encoding='utf_8_sig',index=False)\n",
    "    num_2 = func(2, *popt)    \n",
    "    num_3 = func(3, *popt)  \n",
    "    num_4 = func(4, *popt)  \n",
    "    sum_all = num_2 + num_3 + num_4\n",
    "    num_2_re = round(df1[\"tractcode\"][0] * (num_2/sum_all))\n",
    "    num_3_re = round(df1[\"tractcode\"][0] * (num_3/sum_all))\n",
    "    df_4_1 = dfc[dfc[\"visitors\"] == 4]\n",
    "    # Shuffle the dataframe order\n",
    "    from sklearn.utils import shuffle\n",
    "    df_4 = shuffle(df_4_1, random_state = 5) # set random seed to make everytime the same\n",
    "    df_4.iloc[:num_2_re, 1] = 2 # must use iloc, otherwise, will come up to warning\n",
    "    df_4.iloc[num_2_re : (num_2_re+num_3_re), 1] = 3 # must use iloc, otherwise, will come up to warning\n",
    "    df_no_4 = dfc[dfc[\"visitors\"] != 4]\n",
    "    dfs_1 = [df_4,df_no_4]\n",
    "    df_final = pd.concat(dfs_1)\n",
    "    # df_final.to_csv('C:/Users/ryanz/Desktop/210717-Obesity Estimation/04 Data Sample_NY/03 Safegraph Data/02 NY_Pattern_Natrual Park_712190_3/712190_NY_Natrual Park_data_curve_estimate_last_1.csv',encoding='utf_8_sig',index=False) # output\n",
    "    df_final_1 = df_final.groupby([\"tractcode\"])[\"visitors\"].sum().reset_index()\n",
    "    global df_final_2 \n",
    "    df_final_2 = df_final_1[~df_final_1['tractcode'].str.contains('CA')]\n",
    "    # df_final_2.to_csv('{}/{}curve_estimation_pattern_final.csv'.format(filename_input, folder_name),encoding='utf_8_sig',index=False)\n",
    "    return df_final_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3214999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process summary dataset\n",
    "def summary_process(data, name, filenames1, folder_name):\n",
    "    print(\"Start summary process \" + name)\n",
    "    # read home_panel_summary data\n",
    "    data[[\"census_block_group\"]] = data[[\"census_block_group\"]].astype(str)\n",
    "    data = data[['census_block_group','number_devices_residing']]\n",
    "    data1 = data[~data['census_block_group'].str.contains('CA')].reset_index()\n",
    "    # some census tracts miss 0, add 0\n",
    "    for n in range(len(data1)):\n",
    "        if len(data1['census_block_group'][n]) != 12:\n",
    "            data1['census_block_group'][n] = \"0\" + data1['census_block_group'][n]\n",
    "    # test whether or not they are all sensus block groups, with the 12 number of digits\n",
    "    m = 0\n",
    "    for m in range(len(data1)):\n",
    "        if len(data1['census_block_group'][m]) != 12:\n",
    "            print(name + \" ———> \" + \"home_panel_summary: \" + str(m) + \"is not tract!!!\")\n",
    "    # groupby\n",
    "    data2 = data1.groupby([\"census_block_group\"])[\"number_devices_residing\"].sum().reset_index()\n",
    "    data2 = data2[[\"census_block_group\",\"number_devices_residing\"]]\n",
    "    # delete last digit, change it to census tract\n",
    "    data2['census_block_group'] = data2['census_block_group'].str[:-1] # https://segmentfault.com/q/1010000023190974, this can save a hug of time!\n",
    "    global data3\n",
    "    data3 = data2.groupby([\"census_block_group\"])[\"number_devices_residing\"].sum().reset_index()\n",
    "    # print(name + \" ———> \" + \"pattern and home_panel_summary: \" + \"completed!\")\n",
    "    # data3.to_csv('{}/{}summary{}.csv'.format(filenames1, folder_name, filenames1.split(\"2018\")[1].split(\"-2022-\")[0]),encoding='utf_8_sig',index=False)\n",
    "    return data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78c70947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 document processing\n",
      "Start pattern process 713940-CORE_POI-PATTERNS-2018_01-2022-01-28\n",
      "Start summary process 713940-CORE_POI-PATTERNS-2018_01-2022-01-28\n",
      "2 document processing\n",
      "Start pattern process 713940-CORE_POI-PATTERNS-2018_02-2022-01-28\n",
      "Start summary process 713940-CORE_POI-PATTERNS-2018_02-2022-01-28\n",
      "3 document processing\n",
      "Start pattern process 713940-CORE_POI-PATTERNS-2018_03-2022-01-28\n",
      "Start summary process 713940-CORE_POI-PATTERNS-2018_03-2022-01-28\n",
      "4 document processing\n",
      "Start pattern process 713940-CORE_POI-PATTERNS-2018_04-2022-01-28\n",
      "Start summary process 713940-CORE_POI-PATTERNS-2018_04-2022-01-28\n",
      "5 document processing\n",
      "Start pattern process 713940-CORE_POI-PATTERNS-2018_05-2022-01-28\n",
      "Start summary process 713940-CORE_POI-PATTERNS-2018_05-2022-01-28\n",
      "6 document processing\n",
      "Start pattern process 713940-CORE_POI-PATTERNS-2018_06-2022-01-28\n",
      "Start summary process 713940-CORE_POI-PATTERNS-2018_06-2022-01-28\n",
      "7 document processing\n",
      "Start pattern process 713940-CORE_POI-PATTERNS-2018_07-2022-01-28\n",
      "Start summary process 713940-CORE_POI-PATTERNS-2018_07-2022-01-28\n",
      "8 document processing\n",
      "Start pattern process 713940-CORE_POI-PATTERNS-2018_08-2022-01-28\n",
      "Start summary process 713940-CORE_POI-PATTERNS-2018_08-2022-01-28\n",
      "9 document processing\n",
      "Start pattern process 713940-CORE_POI-PATTERNS-2018_09-2022-01-28\n",
      "Start summary process 713940-CORE_POI-PATTERNS-2018_09-2022-01-28\n",
      "10 document processing\n",
      "Start pattern process 713940-CORE_POI-PATTERNS-2018_10-2022-01-28\n",
      "Start summary process 713940-CORE_POI-PATTERNS-2018_10-2022-01-28\n",
      "11 document processing\n",
      "Start pattern process 713940-CORE_POI-PATTERNS-2018_11-2022-01-28\n",
      "Start summary process 713940-CORE_POI-PATTERNS-2018_11-2022-01-28\n",
      "12 document processing\n",
      "Start pattern process 713940-CORE_POI-PATTERNS-2018_12-2022-01-28\n",
      "Start summary process 713940-CORE_POI-PATTERNS-2018_12-2022-01-28\n",
      "begin curve estimation!\n",
      "713940_Fitness and Recreational Sports centers_completed\n"
     ]
    }
   ],
   "source": [
    "# main \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # this is the only input\n",
    "    filename_input = '../02 Dataset/01 Original Safegraph Data for Dataset 3 (Whole USA)/01 Pattern_Fitness and Recreational Sports centers_713940' # only input\n",
    "    \n",
    "    dir = filename_input\n",
    "    folder_name = filename_input[-6:] + \"_\" + filename_input.split(\"Pattern_\")[1].split(\"_\")[0] + \"_\"\n",
    "    filenames_1 = os.listdir(dir) \n",
    "    filenames = []\n",
    "    for h in range(len(filenames_1)):\n",
    "        filenames.append(filenames_1[h])\n",
    "        \n",
    "    dfs_p = []\n",
    "    dfs_s = []\n",
    "    index = 1\n",
    "    \n",
    "    for name in filenames:\n",
    "        print(str(index) +\" document processing\")\n",
    "        filenames1 = os.path.join(dir,name)\n",
    "        filenames2 = os.path.join(filenames1,\"2022-01-28-23-2018-0\" + str(index) + \"-core_poi-patterns.csv\") \n",
    "        filenames3 = os.path.join(filenames1,\"2022-01-28-23-2018-0\" + str(index) + \"-home_panel_summary.csv\") \n",
    "        \n",
    "        df = pd.read_csv(filenames2)\n",
    "        pattern_process(df, name, filenames1, folder_name)\n",
    "        dfs_p.append(data_visitor1)\n",
    "        \n",
    "        data = pd.read_csv(filenames3, converters = {u'census_block_group':str})\n",
    "        summary_process(data, name, filenames1, folder_name)\n",
    "        dfs_s.append(data3)\n",
    "\n",
    "        index += 1\n",
    "        \n",
    "    pattern_final = pd.concat(dfs_p)\n",
    "    # pattern_final.to_csv('{}/{}pattern_final.csv'.format(filename_input, folder_name),encoding='utf_8_sig',index=False)\n",
    "    curve_estimation(pattern_final, filename_input, folder_name)\n",
    "    \n",
    "    summary_final = pd.concat(dfs_s)\n",
    "    summary_final_1 = summary_final.groupby([\"census_block_group\"])[\"number_devices_residing\"].sum().reset_index()\n",
    "    # summary_final_1.to_csv('{}/{}summary_final.csv'.format(filename_input, folder_name),encoding='utf_8_sig',index=False)\n",
    "    \n",
    "    pattern_summary_final = pd.merge(df_final_2, summary_final_1, left_on = \"tractcode\", right_on = \"census_block_group\", how = \"left\")\n",
    "    pattern_summary_final1 = pattern_summary_final[['tractcode','visitors','number_devices_residing']]\n",
    "    pattern_summary_final1.rename(columns={'tractcode':'tractfps','visitors':'visitors','number_devices_residing':'devices'},inplace=True)\n",
    "    pattern_summary_final1[\"ratio\"] = pattern_summary_final1['visitors']/pattern_summary_final1['devices']\n",
    "    pattern_summary_final1.replace(np.nan, 0, inplace = True)\n",
    "    pattern_summary_final1.replace(np.inf, 0, inplace = True)\n",
    "    pattern_summary_final1[[\"tractfps\"]] = pattern_summary_final1[[\"tractfps\"]].astype(str)\n",
    "    pattern_summary_final1[[\"visitors\"]] = pattern_summary_final1[[\"visitors\"]].astype(int)\n",
    "    pattern_summary_final1[[\"devices\"]] = pattern_summary_final1[[\"devices\"]].astype(int)\n",
    "    pattern_summary_final1[[\"ratio\"]] = pattern_summary_final1[[\"ratio\"]].astype(float)\n",
    "    # pattern_summary_final1.to_csv('{}/{}pattern_summary_last.csv'.format(filename_input, folder_name),encoding='utf_8_sig',index=False)\n",
    "    \n",
    "    # Use geopandas to connect\n",
    "    tract = gpd.read_file('../Data/02 Census Tract Shpfiles of Three Cities/CDC data_Tract_Ob_last.shp')\n",
    "    tract_final = pd.merge(tract, pattern_summary_final1, left_on = \"GEOID\", right_on = \"tractfps\", how = \"left\")\n",
    "    tract_final_1 = tract_final[['GEOID','INTPTLAT','INTPTLON','geometry', 'visitors', 'devices','ratio','obesity_cr','obesity__2','pop']]\n",
    "    tract_final_1.to_file('{}/{}.shp'.format(filename_input, folder_name[:-1]),encoding='utf-8')\n",
    "    tract_final_3 = pd.DataFrame(tract_final_1)\n",
    "    tract_final_4 = tract_final_3[['GEOID','INTPTLAT','INTPTLON', 'visitors', 'devices','ratio','obesity_cr','obesity__2','pop']]\n",
    "    tract_final_4.to_csv('{}/{}.csv'.format(filename_input, folder_name[:-1]),encoding='utf_8_sig',index=False)\n",
    "    print(folder_name + \"completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
